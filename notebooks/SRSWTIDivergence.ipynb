{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "# SRSWTIDivergence\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhreyas/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 14:43:18.380968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741597998.400078  905904 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741597998.405495  905904 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 14:43:18.425138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:datasets:PyTorch version 2.5.0 available.\n",
      "INFO:datasets:TensorFlow version 2.18.0 available.\n",
      "INFO:datasets:JAX version 0.5.2 available.\n",
      "INFO:SRSWTI:SRSWTI Text Analysis Engine initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------  Imports -------------------------------\n",
    "from srswti_axis import SRSWTIDivergence\n",
    "from utils import call_groq_llm, SAMPLE_DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------  Basic Demos -------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence_analyzer = SRSWTIDivergence()\n",
    "docA = SAMPLE_DOCS[0]\n",
    "docB = SAMPLE_DOCS[1]\n",
    "docC = SAMPLE_DOCS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Divergence Example 1 ==\n",
      "Divergence A vs B: 0.44198785358177467 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate divergence between two documents\n",
    "div_score_1 = divergence_analyzer.calculate_divergence(docA, docB)\n",
    "print(\"== Divergence Example 1 ==\")\n",
    "print(\"Divergence A vs B:\", div_score_1, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Divergence Example 2 ==\n",
      "{'divergence_score': 0.5038743775341092, 'cosine_similarity': 0.17952489852905273, 'jensen_shannon_divergence': 0.20086232009308652, 'entropy_p': 6.927951723354903, 'entropy_q': 6.894970119991518, 'text_complexity_1': 0.9192951723354903, 'text_complexity_2': 0.9894970119991517, 'cosine_weight': 0.4890345495572271, 'jsd_weight': 0.510965450442773} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "div_details_2 = divergence_analyzer.calculate_divergence(docA, docC, return_components=True)\n",
    "print(\"== Divergence Example 2 ==\")\n",
    "print(div_details_2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Divergence Example 3 ==\n",
      "{'scores': [0.44198785358177467, 0.5038743775341092], 'similar_texts': ['## Generative AI and Blockchain Integration\\n\\n### 1. NFT Creation and Verification\\n- Generative AI creates unique digital assets that can be minted as NFTs\\n- Smart contracts can verify the authenticity and provenance of AI-generated art\\n\\n### 2. Decentralized AI Training\\n- Blockchain-based platforms enable distributed training of AI models\\n- Token incentives for contributing computing resources or training data\\n\\n### 3. On-chain AI Models\\n- AI models deployed directly on blockchain networks\\n- Enables transparent, verifiable AI inference with immutable records\\n\\n### 4. Governance and Data Marketplaces\\n- DAOs (Decentralized Autonomous Organizations) for governing AI systems\\n- Decentralized data marketplaces for training AI with fair compensation to data owners\\n\\n### 5. Challenges and Considerations\\n- Computational limitations of current blockchain infrastructure\\n- Privacy concerns when combining transparent ledgers with AI capability\\n- Regulatory uncertainty around both generative AI and blockchain technologies'], 'divergent_texts': [\"Quantum computing represents a revolutionary approach to computation that harnesses the principles of quantum mechanics. Unlike classical computers that use bits (0s and 1s), quantum computers utilize quantum bits or qubits that can exist in multiple states simultaneously through superposition. This fundamental difference enables quantum computers to perform certain calculations exponentially faster than their classical counterparts, particularly in areas like factoring large numbers, searching unsorted databases, and simulating quantum systems.\\n\\nThe potential impact of quantum computing extends across numerous fields, including cryptography, drug discovery, materials science, and artificial intelligence. In cryptography specifically, algorithms like Shor's could theoretically break widely-used encryption schemes that rely on the difficulty of factoring large prime numbers. This has spurred the development of post-quantum cryptography methods designed to withstand quantum attacks. Despite these concerns, practical quantum computers capable of breaking current encryption are likely still years away from reality.\\n\\nCurrent quantum computing systems face significant challenges including qubit stability, error correction, and scaling issues. Quantum decoherence—where qubits lose their quantum properties through interaction with the environment—remains a substantial obstacle. Companies like IBM, Google, and startups such as Rigetti are racing to build more powerful quantum processors with increasing qubit counts and better coherence times. While quantum supremacy demonstrations have shown promise, building fault-tolerant, general-purpose quantum computers remains an ongoing scientific and engineering challenge requiring breakthroughs in both hardware and software.\"]} \n",
      "\n",
      "Similar texts: ['## Generative AI and Blockchain Integration\\n\\n### 1. NFT Creation and Verification\\n- Generative AI creates unique digital assets that can be minted as NFTs\\n- Smart contracts can verify the authenticity and provenance of AI-generated art\\n\\n### 2. Decentralized AI Training\\n- Blockchain-based platforms enable distributed training of AI models\\n- Token incentives for contributing computing resources or training data\\n\\n### 3. On-chain AI Models\\n- AI models deployed directly on blockchain networks\\n- Enables transparent, verifiable AI inference with immutable records\\n\\n### 4. Governance and Data Marketplaces\\n- DAOs (Decentralized Autonomous Organizations) for governing AI systems\\n- Decentralized data marketplaces for training AI with fair compensation to data owners\\n\\n### 5. Challenges and Considerations\\n- Computational limitations of current blockchain infrastructure\\n- Privacy concerns when combining transparent ledgers with AI capability\\n- Regulatory uncertainty around both generative AI and blockchain technologies']\n",
      "Divergent texts: [\"Quantum computing represents a revolutionary approach to computation that harnesses the principles of quantum mechanics. Unlike classical computers that use bits (0s and 1s), quantum computers utilize quantum bits or qubits that can exist in multiple states simultaneously through superposition. This fundamental difference enables quantum computers to perform certain calculations exponentially faster than their classical counterparts, particularly in areas like factoring large numbers, searching unsorted databases, and simulating quantum systems.\\n\\nThe potential impact of quantum computing extends across numerous fields, including cryptography, drug discovery, materials science, and artificial intelligence. In cryptography specifically, algorithms like Shor's could theoretically break widely-used encryption schemes that rely on the difficulty of factoring large prime numbers. This has spurred the development of post-quantum cryptography methods designed to withstand quantum attacks. Despite these concerns, practical quantum computers capable of breaking current encryption are likely still years away from reality.\\n\\nCurrent quantum computing systems face significant challenges including qubit stability, error correction, and scaling issues. Quantum decoherence—where qubits lose their quantum properties through interaction with the environment—remains a substantial obstacle. Companies like IBM, Google, and startups such as Rigetti are racing to build more powerful quantum processors with increasing qubit counts and better coherence times. While quantum supremacy demonstrations have shown promise, building fault-tolerant, general-purpose quantum computers remains an ongoing scientific and engineering challenge requiring breakthroughs in both hardware and software.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple documents\n",
    "compare_res_3 = divergence_analyzer.compare_texts([docA, docB, docC], threshold=0.449)\n",
    "print(\"== Divergence Example 3 ==\")\n",
    "print(compare_res_3, \"\\n\")\n",
    "print(f\"Similar texts: {compare_res_3['similar_texts']}\")\n",
    "print(f\"Divergent texts: {compare_res_3['divergent_texts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------  RAG Demos ----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_demo_a(user_query, docs=SAMPLE_DOCS):\n",
    "    \"\"\"\n",
    "    1) Compare doc divergence with user query\n",
    "    2) Find minimal divergence doc\n",
    "    3) Summarize with Groq\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        print(\"[Theorem3 RAG Demo A] No docs.\")\n",
    "        return\n",
    "\n",
    "    best_doc, min_div = None, float('inf')\n",
    "    for d in docs:\n",
    "        score = divergence_analyzer.calculate_divergence(user_query, d)\n",
    "        if score < min_div:\n",
    "            min_div = score\n",
    "            best_doc = d\n",
    "\n",
    "    prompt = (\n",
    "        f\"User query: '{user_query}'\\n\"\n",
    "        f\"Best doc (lowest divergence={min_div}):\\n{best_doc}\\n\"\n",
    "        \"Please explain how it addresses the query.\"\n",
    "    )\n",
    "    llm_response = call_groq_llm(prompt)\n",
    "    print(\"[Theorem3 RAG Demo A] LLM Response:\\n\", llm_response, \"\\n\")\n",
    "\n",
    "def rag_demo_b(docs=SAMPLE_DOCS):\n",
    "    \"\"\"\n",
    "    1) Compare docs pairwise, group them\n",
    "    2) Summarize each group with Groq\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        print(\"[Theorem3 RAG Demo B] No docs.\")\n",
    "        return\n",
    "    res = divergence_analyzer.compare_texts(docs, threshold=0.6)\n",
    "    prompt = (\n",
    "        f\"Compared docs with threshold=0.6. Result:\\n{res}\\n\"\n",
    "        \"Give an overview of grouping.\"\n",
    "    )\n",
    "    llm_response = call_groq_llm(prompt)\n",
    "    print(\"[Theorem3 RAG Demo B] LLM Response:\\n\", llm_response, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Theorem3 RAG Demo A] LLM Response:\n",
      " [Groq LLM] This document addresses the user's query \"I want to learn about quantum computing\" in several ways:\n",
      "\n",
      "1. **Introduction to quantum computing**: The document starts by explaining the fundamental principles of quantum computing, such as the use of qubits, superposition, and the differences between quantum and classical computers. This provides a solid foundation for understanding the basics of quantum computing.\n",
      "\n",
      "2. **Potential impact and applications**: The document discusses the potential impact of quantum computing across various fields, including cryptography, drug discovery, materials science, and artificial intelligence. This gives the user an idea of the potential applications and significance of quantum computing.\n",
      "\n",
      "3. **Challenges and limitations**: The document highlights the current challenges and limitations of quantum computing, such as qubit stability, error correction, and scaling issues. This provides a realistic understanding of the current state of quantum computing and the obstacles that need to be overcome.\n",
      "\n",
      "4. **Current research and development**: The document mentions companies like IBM, Google, and startups like Rigetti, which are actively working on building more powerful quantum processors. This shows the user that quantum computing is an active area of research and development, with many organizations working to advance the field.\n",
      "\n",
      "5. **Explanation of key concepts**: The document explains key concepts, such as quantum decoherence, qubit stability, and error correction, which are essential to understanding the challenges and limitations of quantum computing.\n",
      "\n",
      "Overall, the document provides a comprehensive introduction to quantum computing, covering its principles, potential applications, challenges, and current research. This makes it an ideal resource for someone who wants to learn about quantum computing, as it addresses the user's query by providing a broad overview of the subject. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_demo_a(\"I want to learn about quantum computing\", SAMPLE_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Theorem3 RAG Demo B] LLM Response:\n",
      " [Groq LLM] The provided result shows a grouping of texts based on their similarity, with a threshold of 0.6. The texts are categorized into two groups: 'similar_texts' and 'divergent_texts'.\n",
      "\n",
      "**Similar Texts:**\n",
      "There are two texts that are considered similar, with similarity scores of 0.44198785358177467 and 0.5038743775341092, respectively. These texts are:\n",
      "\n",
      "1. A text discussing the integration of generative AI and blockchain, covering topics such as NFT creation, decentralized AI training, and on-chain AI models.\n",
      "2. A text about quantum computing, its principles, potential impact, and current challenges, including cryptography, drug discovery, and materials science.\n",
      "\n",
      "**Divergent Texts:**\n",
      "There are no texts that are considered divergent, as the 'divergent_texts' list is empty.\n",
      "\n",
      "In summary, the grouping result shows that the two provided texts are considered similar, despite discussing different topics (AI and blockchain vs. quantum computing). This suggests that the similarity threshold of 0.6 is relatively low, allowing for texts with some degree of similarity to be grouped together. If a higher threshold were used, these texts might be considered divergent due to their distinct topics and content. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_demo_b(SAMPLE_DOCS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
