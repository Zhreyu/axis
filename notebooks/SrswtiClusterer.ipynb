{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "# SrswtiClusterer\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhreyas/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 15:10:13.319183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741599613.338179  938910 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741599613.343511  938910 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 15:10:13.363347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:datasets:PyTorch version 2.5.0 available.\n",
      "INFO:datasets:TensorFlow version 2.18.0 available.\n",
      "INFO:datasets:JAX version 0.5.2 available.\n",
      "INFO:SRSWTI:SRSWTI Text Analysis Engine initialized successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wordllama.wordllama:Downloading tokenizer file 'l2_supercat_tokenizer_config.json' from Hugging Face repository 'dleemiller/word-llama-l2-supercat'.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------  Imports -------------------------------\n",
    "from srswti_axis import SrswtiClusterer\n",
    "from utils import call_groq_llm, SAMPLE_DOCS\n",
    "# ---------------------------  Basic 3 Demos -------------------------\n",
    "clusterer = SrswtiClusterer()\n",
    "\n",
    "docs_cluster = [\n",
    "    \"Neural networks are part of ML.\",\n",
    "    \"SVMs are older but still used.\",\n",
    "    \"Reinforcement learning is RL.\",\n",
    "    \"Shakespeare was an English playwright.\",\n",
    "    \"The Beatles were an English rock band.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kmeans_logger:Initialization 1/3: Inertia = 1.76, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:New best inertia: 1.76\n",
      "INFO:kmeans_logger:Initialization 2/3: Inertia = 1.52, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:New best inertia: 1.52\n",
      "INFO:kmeans_logger:Initialization 3/3: Inertia = 1.52, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:KMeans clustering complete. Best inertia: 1.52\n",
      "INFO:kmeans_logger:Total kmeans clustering time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Clusterer 1 ==\n",
      "Labels: [0, 1, 0, 2, 2] Score: 1.5188791751861572 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster the documents using SrswtiClusterer\n",
    "# Parameters:\n",
    "#   - docs_cluster: List of text documents to be clustered\n",
    "#   - k: Number of clusters to form (3 in this case)\n",
    "#   - max_iterations: Maximum iterations for the clustering algorithm (500)\n",
    "labels_c, score_c = clusterer.cluster_documents(docs_cluster, k=3, max_iterations=500)\n",
    "\n",
    "# Print the results\n",
    "print(\"== Clusterer 1 ==\")\n",
    "# Labels show which cluster each document belongs to (0, 1, or 2)\n",
    "# Score represents the quality of clustering \n",
    "print(\"Labels:\", labels_c, \"Score:\", score_c, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Clusterer 2: ML docs ==\n",
      "['Neural networks are part of ML.', 'Reinforcement learning is RL.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract documents labeled as cluster 0 (machine learning related documents)\n",
    "ml_docs = [doc for doc, lbl in zip(docs_cluster, labels_c) if lbl == 0]\n",
    "print(\"== Clusterer 2: ML docs ==\")\n",
    "# Display the extracted machine learning documents\n",
    "print(ml_docs, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------  - 2 Groq LLM + RAG Demos ----------------------\n",
    "\n",
    "def rag_demo_a(docs=SAMPLE_DOCS):\n",
    "    \"\"\"\n",
    "    Demonstrates a basic RAG (Retrieval-Augmented Generation) workflow that:\n",
    "    1. Clusters the input documents into 2 groups\n",
    "    2. Creates a map of documents by cluster\n",
    "    3. Asks an LLM to summarize the content of each cluster\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of text documents to process (defaults to SAMPLE_DOCS)\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the LLM's response to the console\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        print(\"[ RAG Demo A] No docs.\")\n",
    "        return\n",
    "    \n",
    "    # Cluster the documents into 2 groups\n",
    "    cluster_labels, _ = clusterer.cluster_documents(docs, k=2)\n",
    "    \n",
    "    # Group documents by their assigned cluster\n",
    "    cluster_map = {}\n",
    "    for doc, lbl in zip(docs, cluster_labels):\n",
    "        cluster_map.setdefault(lbl, []).append(doc)\n",
    "    \n",
    "    # Create prompt asking LLM to summarize each cluster\n",
    "    prompt = f\"Docs grouped into 2 clusters:\\n{cluster_map}\\nSummarize each cluster.\"\n",
    "    \n",
    "    # Call the LLM with the prompt and print the response\n",
    "    resp = call_groq_llm(prompt)\n",
    "    print(\"[ RAG Demo A] LLM:\\n\", resp, \"\\n\")\n",
    "\n",
    "\n",
    "def rag_demo_b(user_query, docs=SAMPLE_DOCS):\n",
    "    \"\"\"\n",
    "    Demonstrates a query-based RAG workflow that:\n",
    "    1. Clusters the input documents into 2 groups\n",
    "    2. Selects only documents from cluster 0\n",
    "    3. Uses these documents to answer the user's query with an LLM\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The question or query from the user\n",
    "        docs (list): List of text documents to process (defaults to SAMPLE_DOCS)\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the LLM's response to the console\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        print(\"[ RAG Demo B] No docs.\")\n",
    "        return\n",
    "    \n",
    "    # Cluster the documents into 2 groups\n",
    "    cluster_labels, _ = clusterer.cluster_documents(docs, k=2)\n",
    "    \n",
    "    # Select only documents from cluster 0\n",
    "    chosen_docs = [doc for doc, lbl in zip(docs, cluster_labels) if lbl == 0]\n",
    "    \n",
    "    # Create prompt with user query and selected documents\n",
    "    prompt = (\n",
    "        f\"User query: {user_query}\\nDocs in cluster 0:\\n{chosen_docs}\\n\"\n",
    "        \"Answer only with these docs.\"\n",
    "    )\n",
    "    \n",
    "    # Call the LLM with the prompt and print the response\n",
    "    resp = call_groq_llm(prompt)\n",
    "    print(\"[ RAG Demo B] LLM:\\n\", resp, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kmeans_logger:Initialization 1/3: Inertia = 0.69, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:New best inertia: 0.69\n",
      "INFO:kmeans_logger:Initialization 2/3: Inertia = 0.69, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:Initialization 3/3: Inertia = 0.61, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:New best inertia: 0.61\n",
      "INFO:kmeans_logger:KMeans clustering complete. Best inertia: 0.61\n",
      "INFO:kmeans_logger:Total kmeans clustering time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Theorem7 RAG Demo A] LLM:\n",
      " [Groq LLM] **Cluster 0: Neural Networks and AI**\n",
      "\n",
      "This cluster focuses on the concepts and applications of neural networks, a key area of artificial intelligence. The documents cover the structure, learning processes, and types of neural networks, including feedforward, convolutional, and recurrent neural networks. The applications of neural networks in image and speech recognition, natural language processing, game playing, and medical diagnosis are also discussed. The challenges of neural networks, such as overfitting and interpretability, are mentioned, but the overall tone is positive, highlighting the potential of neural networks to drive innovations in various fields.\n",
      "\n",
      "**Cluster 1: Emerging Technologies - Blockchain, Generative AI, and Quantum Computing**\n",
      "\n",
      "This cluster explores the intersection of emerging technologies, including blockchain, generative AI, and quantum computing. The documents discuss the potential applications of these technologies, such as creating unique digital assets, decentralized AI training, and decentralized data marketplaces. The challenges and considerations of these technologies, including computational limitations, privacy concerns, and regulatory uncertainty, are also addressed. Additionally, the cluster touches on the potential impact of quantum computing on cryptography and other fields, highlighting the ongoing scientific and engineering challenges in building more powerful and stable quantum computers. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_demo_a(SAMPLE_DOCS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kmeans_logger:Initialization 1/3: Inertia = 0.61, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:New best inertia: 0.61\n",
      "INFO:kmeans_logger:Initialization 2/3: Inertia = 0.61, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:Initialization 3/3: Inertia = 0.61, Iterations = 5, Time = 0.00 seconds\n",
      "INFO:kmeans_logger:KMeans clustering complete. Best inertia: 0.61\n",
      "INFO:kmeans_logger:Total kmeans clustering time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Theorem7 RAG Demo B] LLM:\n",
      " [Groq LLM] There are several AI approaches, specifically within the realm of Neural Networks. These include:\n",
      "\n",
      "1. **Feedforward Neural Networks**: Information flows in one direction, making them suitable for simple applications.\n",
      "2. **Convolutional Neural Networks (CNNs)**: Specialized for image processing, CNNs are widely used in computer vision tasks.\n",
      "3. **Recurrent Neural Networks (RNNs)**: Handle sequential data with memory, making them suitable for tasks like speech recognition and natural language processing.\n",
      "4. **Transformers**: An advanced architecture for natural language processing, transformers have shown impressive results in tasks like language translation and text generation.\n",
      "\n",
      "These AI approaches have various applications, including:\n",
      "\n",
      "* Image and speech recognition\n",
      "* Natural language processing\n",
      "* Game playing and decision making\n",
      "* Medical diagnosis and predictions\n",
      "\n",
      "However, neural networks also come with challenges like overfitting, computational intensity, and interpretability. Despite these challenges, they remain a key driver of innovations in artificial intelligence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_demo_b(\"Explain AI approaches\", SAMPLE_DOCS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
